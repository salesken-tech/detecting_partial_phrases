{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# preprocessor text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_token_allowed(token):\n",
    "    \"\"\"\n",
    "    This funtion removes the stop words and punctuations \n",
    "    \"\"\"\n",
    "    if (not token or not token.string.strip() or token.is_stop \n",
    "        or token.is_punct):\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def lematizer(token):\n",
    "    return token.lemma_.strip().lower()\n",
    "\n",
    "def sentence_preprocessor(sentence):\n",
    "    doc=nlp(sentence)\n",
    "    return [lematizer(token) for token in doc if is_token_allowed(token)]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=sentence_preprocessor(\"I am looking for a three bedroom apartment with a huge lawn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=' '.join([word for word in x ]).strip()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'look bedroom apartment huge lawn'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tree parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sen=\"I am looking for a three bedroom apartment with a big lawn\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc=nlp(sen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#extracting children nodes for noun\n",
    "def flatten_tree(tree):\n",
    "    return ''.join([token.text_with_ws for token in list(tree)]).strip()\n",
    "\n",
    "np=[]\n",
    "for tok in doc:\n",
    "    if tok.pos_ == \"NOUN\":\n",
    "        np.append(flatten_tree(tok.subtree))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['three bedroom', 'a three bedroom apartment with a big lawn', 'a big lawn']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bedroom\n",
      "bedroom apartment big lawn\n",
      "big lawn\n"
     ]
    }
   ],
   "source": [
    "#clean text\n",
    "for item in np:\n",
    "    print(' '.join([word for word in sentence_preprocessor(item)]).strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# extracting verb_phrases and noun phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textacy\n",
    "from nltk.tree import Tree\n",
    "sen=\"I am looking for a three bedroom apartment with a big lawn\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I\n",
      "a three bedroom apartment\n",
      "a big lawn\n"
     ]
    }
   ],
   "source": [
    "pattern = r'(<VERB>?<ADV>*<VERB>+)'\n",
    "# verb_clause_pattern = r'<VERB>*<ADV>*<PART>*<VERB>+<PART>*'\n",
    "# doc=textacy.make_spacy_doc(sen,lang='en_core_web_sm')\n",
    "doc = nlp(sen)\n",
    "for item in doc.noun_chunks:\n",
    "    print(item)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rolling window "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Byjus, is, the, worlds, largest, edtech, company]\n",
      "!!!!!!!!!!!!!!!!!!!!!!\n",
      "[Byjus is, is the, the worlds, worlds largest, largest edtech, edtech company]\n",
      "!!!!!!!!!!!!!!!!!!!!!!\n",
      "[Byjus is the, is the worlds, the worlds largest, worlds largest edtech, largest edtech company]\n",
      "!!!!!!!!!!!!!!!!!!!!!!\n",
      "[Byjus is the worlds, is the worlds largest, the worlds largest edtech, worlds largest edtech company]\n",
      "!!!!!!!!!!!!!!!!!!!!!!\n",
      "[Byjus is the worlds largest, is the worlds largest edtech, the worlds largest edtech company]\n",
      "!!!!!!!!!!!!!!!!!!!!!!\n",
      "[Byjus is the worlds largest edtech, is the worlds largest edtech company]\n",
      "!!!!!!!!!!!!!!!!!!!!!!\n",
      "[Byjus is the worlds largest edtech company]\n",
      "!!!!!!!!!!!!!!!!!!!!!!\n",
      "TIme=0.0006246566772460938\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "sen=\"Byjus is the worlds largest edtech company\"\n",
    "doc=textacy.make_spacy_doc(sen,lang='en_core_web_sm')\n",
    "s=time.time()\n",
    "for i in range(len(sen.split())):\n",
    "    n_gram = textacy.extract.ngrams(doc,n=i+1, filter_stops=False)\n",
    "    print([item for item in n_gram])\n",
    "    \n",
    "    print(\"!!!!!!!!!!!!!!!!!!!!!!\")\n",
    "print(\"TIme={}\".format(time.time()-s))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Byjus is\n",
      "is the\n",
      "the world\n",
      "world's\n",
      "'s largest\n",
      "largest ed\n",
      "tech company\n"
     ]
    }
   ],
   "source": [
    "for item in verb_phrase:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
